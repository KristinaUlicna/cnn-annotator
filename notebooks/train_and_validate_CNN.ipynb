{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a CNN for cell cycle state classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using colab, install cellx library and make log and data folders\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install -q git+git://github.com/quantumjot/cellx.git\n",
    "    !mkdir logs\n",
    "    !mkdir train\n",
    "    !mkdir test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and set up hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellx.layers import Encoder2D\n",
    "from cellx.tools.dataset import build_dataset\n",
    "from cellx.tools.dataset import write_dataset\n",
    "from cellx.augmentation.utils import append_conditional_augmentation, augmentation_label_handler\n",
    "from cellx.callbacks import tensorboard_confusion_matrix_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./train\"\n",
    "TEST_PATH = \"./test\"\n",
    "TRAIN_FILE = os.path.join(TRAIN_PATH, 'CNN_train.tfrecord')\n",
    "TEST_FILE = os.path.join(TEST_PATH, 'CNN_test.tfrecord')\n",
    "LABELS = [\"Interphase\", \"Prometaphase\", \"Metaphase\", \"Anaphase\", \"Apoptosis\"]\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20_000\n",
    "TRAINING_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "LOG_ROOT = './logs'\n",
    "LOG_DIR = os.path.join(LOG_ROOT, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training/testing data and generate TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(\n",
    "    root, \n",
    "    filename,\n",
    "    labels=LABELS\n",
    "):\n",
    "    \n",
    "    _images = []\n",
    "    _labels = []\n",
    "    \n",
    "    # find the zip files:\n",
    "    zipfiles = [os.path.join(root, f) for f in os.listdir(root) if f.endswith(\".zip\") and f.startswith(\"annotation_\")]\n",
    "    \n",
    "    for zfn in zipfiles:\n",
    "        print(f\"Loading file: {zfn}\")\n",
    "        with zipfile.ZipFile(zfn, 'r') as zip_data:\n",
    "            files = zip_data.namelist()\n",
    "\n",
    "            for numeric_label, label in enumerate(labels):\n",
    "\n",
    "                patch_files = [f for f in files if f.endswith(\".tif\") and f.startswith(label.capitalize())]\n",
    "                images = [imread(zip_data.open(f)) for f in patch_files]\n",
    "                images_resized = [resize(img, (64, 64), preserve_range=True) for img in images]\n",
    "\n",
    "                _images += images_resized\n",
    "                _labels += [numeric_label] * len(images_resized)\n",
    "\n",
    "                \n",
    "    images_arr = np.stack(_images, axis=0)[..., np.newaxis]\n",
    "    labels_arr = np.stack(_labels, axis=0)\n",
    "    \n",
    "    print(f\"Total images: {images_arr.shape[0]}\")\n",
    "    write_dataset(filename, images_arr.astype(np.uint8), labels=labels_arr.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(TRAIN_PATH, TRAIN_FILE)\n",
    "create_tf_record(TEST_PATH, TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple CNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = K.layers.Input(shape=(64, 64, 1))\n",
    "x = Encoder2D(layers=[8, 16, 32, 64, 128])(img)\n",
    "x = K.layers.Flatten()(x)\n",
    "x = K.layers.Dense(256, activation=\"relu\")(x)\n",
    "x = K.layers.Dropout(0.2)(x)\n",
    "logits = K.layers.Dense(5, activation=\"linear\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = K.Model(inputs=img, outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up some augmentations to be used while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def normalize(img):\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    # clip to 4 standard deviations\n",
    "    img = tf.clip_by_value(img, -4., 4.)\n",
    "    tf.debugging.check_numerics(img, \"Image contains NaN\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def augment(img):\n",
    "    boundary_augmentation=True\n",
    "    if boundary_augmentation:\n",
    "        # this will randomly simulate the cropping that occurs at the edge of\n",
    "        # an image volume\n",
    "\n",
    "        vignette = np.ones((64, 64, 1), dtype=np.float32)\n",
    "        width = np.random.randint(0,30)\n",
    "        vignette[:,:width,...] = 0\n",
    "\n",
    "        img = tf.cond(pred=tf.random.uniform(shape=())<0.05,\n",
    "                true_fn=lambda: tf.multiply(img, vignette),\n",
    "                false_fn=lambda: img)\n",
    "\n",
    "    # do some data augmentation\n",
    "    k = tf.random.uniform(maxval=3, shape=(), dtype=tf.int32)\n",
    "    img = tf.image.rot90(img, k=k)\n",
    "\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def random_contrast(x):\n",
    "    return tf.image.random_contrast(x, 0.3, 1.0)\n",
    "\n",
    "@augmentation_label_handler\n",
    "def random_brightness(x):\n",
    "    return tf.image.random_brightness(x, 0.3, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the training dataset, with random augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = build_dataset(TRAIN_FILE, read_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(augment)\n",
    "dataset = append_conditional_augmentation(dataset, [random_contrast, random_brightness])\n",
    "dataset = dataset.map(normalize)\n",
    "dataset = dataset.shuffle(buffer_size=BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the test dataset, without augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = build_dataset(TEST_FILE, read_label=True)\n",
    "test_dataset = test_dataset.map(normalize)\n",
    "test_dataset = test_dataset.take(-1).as_numpy_iterator()\n",
    "\n",
    "test_images, test_labels = zip(*list(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tensorboard callbacks to monitor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = K.callbacks.TensorBoard(log_dir=LOG_DIR)\n",
    "confusion_matrix_callback = tensorboard_confusion_matrix_callback(\n",
    "    model, \n",
    "    np.asarray(test_images), \n",
    "    test_labels,\n",
    "    LOG_DIR,\n",
    "    class_names=LABELS,\n",
    "    is_binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = K.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\", loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, train the model and evaluate performance using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir $LOG_ROOT --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    dataset, \n",
    "    steps_per_epoch=BUFFER_SIZE//BATCH_SIZE, \n",
    "    epochs=TRAINING_EPOCHS, \n",
    "    callbacks=[tensorboard_callback, confusion_matrix_callback],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

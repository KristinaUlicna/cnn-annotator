{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Testing for Cell Cycle State Classification\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This notebook allows you to take the convolutional neural network (CNN) that you trained in the previous notebook and evaluate its performance on a set of previously unseen single-cell image patches. Follow the step-wise instructions to proceed with testing the network.\n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "1. You are using the virtual environment of the [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb \"Google Colaboratory\"). To be able to test the neural network, you must first **import images not used during training** into the folders to source from. Please follow the instructions after executing the first cell of this notebook.\n",
    "\n",
    "2. If using Google Colab: This session will 'timeout' if you do not interact with it. It's 90 minutes if you close the browser or 12 hours if you keep the browser open. Additionally, if you close your browser with a code cell is running, if that same cell has not finished, when you reopen the browser it will still be running (the current executing cell keeps running even after browser is closed). Please visit this [StackOverflow](https://stackoverflow.com/questions/54057011/google-colab-session-timeout \"Google Colab Session Timeout\") discussion for more details.\n",
    "\n",
    "\n",
    "### Running Instructions:\n",
    "\n",
    "1. Execute the first cell containing code below, which will install the CellX library & create local directories in the environment of the virtual machine. (Note: This virtual environment is different from the one created for the Training notebook.)\n",
    "\n",
    "2. The executed first cell will print ```Building wheel for cellx (setup.py) ... done```. Click on the ``` 📁``` folder icon located on the left-side dashboard of the Colab notebook. You should now see 4 subfolders in this directory: \"sample_data\" (default), \"logs\" \"train\" and \"test\" folder, which should all be empty.\n",
    "\n",
    "3. At this point, you should **manually move your 'annotation_XXX.zip' files into the \"train\" and \"test\" folders**. Doing so will allow the image patch data to be processed, divided into categories and used for model training & predictions.\n",
    "\n",
    "4. You can now now run the entire notebook by clicking on ```Runtime``` > ```Run``` in the upper main dashboard. Re-running the initial cell will fail to create the \"logs\" \"train\" and \"test\" folders as those are already in the directory. \n",
    "\n",
    "5. Prior to training of the model, this notebook will distribute the image patch data into the training & testing sets and introduce data augmentations. The notebook will ultimately train the neural network based on the hyperparameters you've set up.\n",
    "\n",
    "6. During training, you can actively visualise what the network is doing via [TensorBoard](https://www.tensorflow.org/tensorboard/get_started \"TensorFlow || Tensorboard\"), a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.\n",
    "\n",
    "---\n",
    "\n",
    "**Happy training!**\n",
    "\n",
    "*Your [CellX](http://lowe.cs.ucl.ac.uk/cellx.html \"Lowe Lab @ UCL\") team*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the CellX library & create subdirectories in the virtual machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using colab, install cellx library and make log and data folders\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install -q git+git://github.com/quantumjot/cellx.git\n",
    "    !mkdir logs\n",
    "    !mkdir train\n",
    "    !mkdir test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and CellX toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.special import softmax\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cellx.layers import Encoder2D\n",
    "from cellx.tools.confusion import plot_confusion_matrix\n",
    "from cellx.tools.dataset import build_dataset\n",
    "from cellx.tools.dataset import write_dataset\n",
    "from cellx.tools.projection import ManifoldProjection2D\n",
    "\n",
    "from cellx.augmentation.utils import append_conditional_augmentation, augmentation_label_handler\n",
    "from cellx.callbacks import tensorboard_confusion_matrix_callback\n",
    "from cellx.core import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths & class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./train\"\n",
    "TEST_PATH = \"./test\"\n",
    "TRAIN_FILE = os.path.join(TRAIN_PATH, 'CNN_train.tfrecord')\n",
    "TEST_FILE = os.path.join(TEST_PATH, 'CNN_test.tfrecord')\n",
    "LABELS = [\"Interphase\", \"Prometaphase\", \"Metaphase\", \"Anaphase\", \"Apoptosis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TensorFlow Record (TFRecord) files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(\n",
    "    root, \n",
    "    filename,\n",
    "    labels=LABELS\n",
    "):\n",
    "    \n",
    "    _images = []\n",
    "    _labels = []\n",
    "    \n",
    "    # find the zip files:\n",
    "    zipfiles = [os.path.join(root, f) for f in os.listdir(root) if f.endswith(\".zip\") and f.startswith(\"annotation_\")]\n",
    "    \n",
    "    if len(zipfiles) == 0:\n",
    "        raise Exception(\"Warning, no 'annotation' zip files found in the directory.\")\n",
    "    \n",
    "    for zfn in zipfiles:\n",
    "        print(f\"Loading file: {zfn}\")\n",
    "        with zipfile.ZipFile(zfn, 'r') as zip_data:\n",
    "            files = zip_data.namelist()\n",
    "\n",
    "            for numeric_label, label in enumerate(labels):\n",
    "\n",
    "                patch_files = [f for f in files if f.endswith(\".tif\") and f.startswith(label.capitalize())]\n",
    "                images = [plt.imread(zip_data.open(f)) for f in patch_files]\n",
    "                images_resized = [resize(img, (64, 64), preserve_range=True) for img in images]\n",
    "\n",
    "                _images += images_resized\n",
    "                _labels += [numeric_label] * len(images_resized)\n",
    "\n",
    "                \n",
    "    images_arr = np.stack(_images, axis=0)[..., np.newaxis]\n",
    "    labels_arr = np.stack(_labels, axis=0)\n",
    "    \n",
    "    print(f\"Total images: {images_arr.shape[0]}\")\n",
    "    write_dataset(filename, images_arr.astype(np.uint8), labels=labels_arr.astype(np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: \n",
    "\n",
    "**Prior to calling the function to create the TFRecords files:**\n",
    "\n",
    "You need to manually drag the annotation_XXX.zip files into the newly created folders. If you are working in the Google Colab environment, click on the folder icon at the left-side dashboard, which should now contain the 'logs', 'train' and 'test' directories. They should be empty until you drag your annotation files into them.\n",
    "\n",
    "Once the files have been imported, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(TRAIN_PATH, TRAIN_FILE)\n",
    "create_tf_record(TEST_PATH, TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the \"load_model\" function from the CellX library, we can import models without needing to specify the CellX custom layers that had been used to build them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model'\n",
    "model = load_model('{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the test dataset, without augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def normalize(img):\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    # clip to 4 standard deviations\n",
    "    img = tf.clip_by_value(img, -4., 4.)\n",
    "    tf.debugging.check_numerics(img, \"Image contains NaN\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = build_dataset(TEST_FILE, read_label=True)\n",
    "test_dataset = test_dataset.map(normalize)\n",
    "test_dataset = test_dataset.take(-1).as_numpy_iterator()\n",
    "\n",
    "test_images, test_labels = zip(*list(test_dataset))\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model on the Test_Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of test_images should be (N,64,64,1) where N is the number of individual images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_images) # shape = (N,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'softmax' function transforms test_predictions into an array of scores for each class for each instance in the testing set. Across classes, the scores sum to one. The class associated with the highest score is the model's 'prediction'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = softmax(test_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on Testing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample N images out of the testing set to check the model's predictions on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_testing_predictions(\n",
    "    image_num_list, # indices of the examples in the testing set to be shown\n",
    "    test_images\n",
    "):\n",
    "    plt.figure(figsize=(10,3*(int(len(image_num_list)/5)+1)))\n",
    "    plt.suptitle('Predictions',fontsize=25,x=0.5,y=0.95)\n",
    "    for image_num in image_num_list:\n",
    "        plt.subplot(int(len(image_num_list)/5)+1,5,np.where(image_num_list==image_num)[0]+1)\n",
    "        plt.imshow(test_images[image_num,:,:,0])\n",
    "        plt.title('Image {}'.format(image_num+1))\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "        plt.xlabel(LABELS[np.argmax(test_predictions[image_num])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_testing_predictions(np.arange(20,test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next calculate the \"precision\", \"recall\" and \"F1 score\" metrics for each class, as well as the \"confusion matrix\" for the CNN's performance on the testing set. The three metrics are calculated using the number of \"false positive\", \"true positive\" and \"false negative\" predictions for each class.\n",
    "- The \"precision\" of class X is calculated by $$precision(X) = \\frac{No.\\;of\\;true\\;positives}{No.\\;of\\;true\\;positives+No.\\;of\\;false\\;positives}$$\n",
    "- The \"recall\" of class X is calculated by $$recall(X) = \\frac{No.\\;of\\;true\\;positives}{No.\\;of\\;true\\;positives+No.\\;of\\;false\\;negatives}$$\n",
    "- The \"F1 score\" of class X is calculated by $$F1(X) = 2*\\frac{precision(X)*recall(X)}{precision(X)+recall(X)}$$\n",
    "<br>\n",
    "\n",
    "The \"confusion matrix\" is a table that visually represents the performance of a network on a testing set. The number shown in row A and column B is the number of testing examples of ground-truth class A that have been predicted as belonging to class B by the network.\n",
    "\n",
    "Reading resource for confusion matrices: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = model.evaluate(test_images, test_labels)\n",
    "\n",
    "test_confusion_matrix = confusion_matrix(test_labels,np.argmax(test_predictions,axis=1))\n",
    "test_confusion_matrix_plot = plot_confusion_matrix(test_confusion_matrix,LABELS)\n",
    "test_confusion_matrix_plot.show()\n",
    "\n",
    "print('Testing Accuracy = ',accuracy)\n",
    "print('Testing Loss = ',loss)\n",
    "\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(test_labels,np.argmax(test_predictions,axis=1))\n",
    "print('Testing Precision = ',precision)\n",
    "print('Testing Recall = ',recall)\n",
    "print('Testing F1 Score = ',fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the below cell, we notice that the model output is an array of 2 dimensions: \n",
    "* 1st dimension corresponds to the number of test images used \n",
    "* 2nd dimension represents the number of predefined classes that images can be classified as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use UMAP to visualise the network's classfication performance by embedding the predictions into a lower 2D space, more concretly this means that we reduce the 2nd dimension of 5 to 2 while attempting to match as closely as possible the data structure of higher- vs. lower-dimensional space.\n",
    "\n",
    "We first define our parameters of choice. In this simple example, we chose to only modify the following ones:\n",
    "* `n_neighbors` - determines the size of the local neighbourhood that UMAP should focus on, low values will make it emphasise on local structure whereas high values on global structure\n",
    "* `n_epochs` - similarly to training the CNN, this determines the number of rounds that the UMAP model will be trained for\n",
    "* `random_state` - UMAP is a stochastic algorithm, so we need to set a fixed seed to ensure that the results are reproducible across different runs\n",
    "\n",
    "Feel free to adjust the parameters and see how the below projection! You can read up on the most important parameters [here](https://umap-learn.readthedocs.io/en/latest/parameters.html#min-dist).\n",
    "\n",
    "If you're interested in reading about how UMAP works, [see here](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP parameters\n",
    "nbs = 5\n",
    "eps = 50\n",
    "rnd = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a UMAP model with the defined parameters. The full configuration of the UMAP model will be printed out with all the parameter values used, including the ones modified above. \n",
    "\n",
    "Note:`verbose=True`enables written feedback to the user while UMAP is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = UMAP(n_neighbors=nbs, n_epochs=eps, random_state=rnd, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.fit(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image patch projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By projecting the test images corresponding to the test predictions on top of the UMAP embedding, we can visually assess whether single-cell patches of the same cell state cluster together in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert single-channel test images to rgb three-channel images\n",
    "print(f\"shape of test_images: {test_images.shape}\")\n",
    "rgb_images = np.concatenate([test_images]*3, axis=-1)\n",
    "print(f\"shape of rgb_images: {rgb_images.shape}\")\n",
    "# normalise image values to 0-1 range (Min-Max scaling) & convert to 8-bit\n",
    "rgb_images = ((rgb_images-np.min(rgb_images))/(np.ptp(rgb_images)) * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the grid of image patches corresponding to the UMAP embedding. (This is similar to a 2D histogram where points on a same grid cell will be binned together.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = ManifoldProjection2D(rgb_images)\n",
    "img_grid, heatmap, delimiters = projection(mapper.embedding_, components=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a figure to show the image projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "im = plt.imshow(img_grid,\n",
    "           origin=\"lower\",\n",
    "          #  extent=delimiters, \n",
    "           cmap=\"gray\",)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
